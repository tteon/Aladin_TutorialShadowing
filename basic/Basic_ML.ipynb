{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Manipulation_1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D Array with Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 2. 3. 4. 5. 6.]\n",
      "Rank of t:  1\n",
      "Shape of t:  (7,)\n"
     ]
    }
   ],
   "source": [
    "t = np.array([0., 1., 2., 3., 4., 5., 6.,])\n",
    "print(t)\n",
    "print('Rank of t: ', t.ndim)\n",
    "print('Shape of t: ', t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t[0] t[1] t[-1]) =  0.0 1.0 6.0\n",
      "t[2:5] t[4:-1]) =  [2. 3. 4.] [4. 5.]\n"
     ]
    }
   ],
   "source": [
    "print('t[0] t[1] t[-1]) = ', t[0], t[1], t[-1])\n",
    "print('t[2:5] t[4:-1]) = ', t[2:5], t[4:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Array with NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  2.]\n",
      " [ 3.  4.  5.]\n",
      " [ 6.  7.  8.]\n",
      " [ 9. 10. 11.]]\n",
      "2\n",
      "(4, 3)\n"
     ]
    }
   ],
   "source": [
    "t = np.array([[0., 1., 2.], [3., 4., 5.], [6.,7.,8.], [9.,10.,11.]])\n",
    "print(t)\n",
    "print(t.ndim)\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D Array with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3., 4., 5., 6.])\n",
      "1\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "tensor(0.) tensor(1.) tensor(6.)\n",
      "tensor([1., 2.]) tensor([])\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([0., 1., 2., 3., 4., 5., 6.])\n",
    "print(t)\n",
    "print(t.dim())\n",
    "print(t.shape)\n",
    "print(t.size())\n",
    "print(t[0], t[1], t[-1])\n",
    "print(t[1:3], t[-1:-3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Array with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.],\n",
      "        [ 7.,  8.,  9.],\n",
      "        [10., 11., 12.]])\n",
      "2\n",
      "torch.Size([4, 3])\n",
      "tensor([ 2.,  5.,  8., 11.])\n",
      "tensor([4., 5., 6.])\n",
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([[1.,2.,3.,],\n",
    "                     [4.,5.,6.,],\n",
    "                     [7.,8.,9.,],\n",
    "                     [10.,11.,12.,]])\n",
    "print(t)\n",
    "print(t.dim())\n",
    "print(t.size())\n",
    "print(t[:, 1])\n",
    "print(t[1, :])\n",
    "print(t[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 5.]])\n"
     ]
    }
   ],
   "source": [
    "# Same shape\n",
    "m1 = torch.FloatTensor([[3, 3]])\n",
    "m2 = torch.FloatTensor([[2, 2]])\n",
    "print(m1 + m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 5.]])\n"
     ]
    }
   ],
   "source": [
    "# Vector + scalar\n",
    "m1 = torch.FloatTensor([[1, 2]])\n",
    "m2 = torch.FloatTensor([[3,]])\n",
    "print(m1 + m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 4.],\n",
      "        [2., 3.]])\n"
     ]
    }
   ],
   "source": [
    "# 2 x 1 vector + 1 x 2 vector\n",
    "m1 = torch.FloatTensor([[1,2]])\n",
    "m2 = torch.FloatTensor([[2], [1]])\n",
    "print(m1 + m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiplication vs. Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Mul vs Matmul\n",
      "========================================\n",
      "Shape of Matrix 1:  torch.Size([2, 2])\n",
      "Shape of Matrix 2:  torch.Size([2, 1])\n",
      "tensor([[ 5.],\n",
      "        [11.]])\n",
      "Shape of Matrix 1:  torch.Size([2, 2])\n",
      "Shape of Matrix 2:  torch.Size([2, 1])\n",
      "tensor([[1., 2.],\n",
      "        [6., 8.]])\n",
      "tensor([[1., 2.],\n",
      "        [6., 8.]])\n"
     ]
    }
   ],
   "source": [
    "print('=' * 40)\n",
    "print('Mul vs Matmul')\n",
    "print('=' * 40)\n",
    "\n",
    "m1 = torch.FloatTensor([[1, 2], [3,4]])\n",
    "m2 = torch.FloatTensor([[1], [2]])\n",
    "\n",
    "print('Shape of Matrix 1: ', m1.shape) # 2 x 2\n",
    "print('Shape of Matrix 2: ', m2.shape) # 2 x 2\n",
    "print(m1.matmul(m2)) # 2 x 1\n",
    "\n",
    "m1 = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "m2 = torch.FloatTensor([[1], [2]])\n",
    "print('Shape of Matrix 1: ', m1.shape)\n",
    "print('Shape of Matrix 2: ', m2.shape)\n",
    "print(m1 * m2)\n",
    "print(m1.mul(m2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5000)\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([1,2])\n",
    "print(t.mean())\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can only calculate the mean of floating types. Got Long instead.\n"
     ]
    }
   ],
   "source": [
    "# Can't use mean() on integers\n",
    "t = torch.LongTensor([1, 2])\n",
    "try:\n",
    "    print(t.mean())\n",
    "except Exception as exc:\n",
    "    print(exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yo can also use 't.mean' for higher rank tensors to get mean of all elements, or mean by particular dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.FloatTensor([[1 , 2], [3 , 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method mean of Tensor object at 0x7fd638096048>\n",
      "tensor([2., 3.])\n",
      "tensor([1.5000, 3.5000])\n",
      "tensor([1.5000, 3.5000])\n"
     ]
    }
   ],
   "source": [
    "print(t.mean)\n",
    "print(t.mean(dim=0))\n",
    "print(t.mean(dim=1))\n",
    "print(t.mean(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method sum of Tensor object at 0x7fd638096048>\n",
      "tensor([4., 6.])\n",
      "tensor([3., 7.])\n",
      "tensor([3., 7.])\n"
     ]
    }
   ],
   "source": [
    "print(t.sum)\n",
    "print(t.sum(dim=0))\n",
    "print(t.sum(dim=1))\n",
    "print(t.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max and Argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.)\n",
      "torch.return_types.max(\n",
      "values=tensor([3., 4.]),\n",
      "indices=tensor([1, 1]))\n",
      "Max:  tensor([3., 4.])\n",
      "ArgMax:  tensor(1)\n",
      "torch.return_types.max(\n",
      "values=tensor([2., 4.]),\n",
      "indices=tensor([1, 1]))\n",
      "torch.return_types.max(\n",
      "values=tensor([2., 4.]),\n",
      "indices=tensor([1, 1]))\n"
     ]
    }
   ],
   "source": [
    "print(t.max())\n",
    "print(t.max(dim=0))\n",
    "print('Max: ', t.max(dim=0)[0])\n",
    "print('ArgMax: ', t.argmax(dim=0)[1])\n",
    "print(t.max(dim=1))\n",
    "print(t.max(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Manipulation_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View (Reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "t = np.array([[[0, 1, 2],\n",
    "             [3, 4, 5]],\n",
    "            \n",
    "            [[6, 7, 8],\n",
    "            [9, 10, 11]]])\n",
    "ft = torch.FloatTensor(t)\n",
    "print(ft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.],\n",
      "        [ 3.,  4.,  5.],\n",
      "        [ 6.,  7.,  8.],\n",
      "        [ 9., 10., 11.]])\n",
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "print(ft.view([-1, 3]))\n",
    "print(ft.view([-1, 3]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.,  1.,  2.]],\n",
      "\n",
      "        [[ 3.,  4.,  5.]],\n",
      "\n",
      "        [[ 6.,  7.,  8.]],\n",
      "\n",
      "        [[ 9., 10., 11.]]])\n",
      "torch.Size([4, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "print(ft.view([-1, 1, 3]))\n",
    "print(ft.view([-1, 1, 3]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Squeeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "ft = torch.FloatTensor([[0], [1], [2]])\n",
    "print(ft)\n",
    "print(ft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2.])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "print(ft.squeeze())\n",
    "print(ft.squeeze().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsqueeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "ft = torch.Tensor([0, 1, 2])\n",
    "print(ft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.]])\n",
      "torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "print(ft.unsqueeze(0))\n",
    "print(ft.unsqueeze(0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.]])\n",
      "torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "print(ft.view(1, -1))\n",
    "print(ft.view(1, -1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "print(ft.unsqueeze(1))\n",
    "print(ft.unsqueeze(1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "print(ft.unsqueeze(-1))\n",
    "print(ft.unsqueeze(-1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type Casting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "lt = torch.LongTensor([1, 2, 3, 4])\n",
    "print(lt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4.])\n"
     ]
    }
   ],
   "source": [
    "print(lt.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 0, 1], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "bt = torch.ByteTensor([True, False, False, True])\n",
    "print(bt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 0, 1])\n",
      "tensor([1., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "print(bt.long())\n",
    "print(bt.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "y = torch.FloatTensor([[5, 6], [7, 8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.],\n",
      "        [7., 8.]])\n",
      "torch.Size([4, 2])\n",
      "tensor([[1., 2., 5., 6.],\n",
      "        [3., 4., 7., 8.]])\n",
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "print(torch.cat([x, y], dim=0))\n",
    "a=torch.cat([x,y])\n",
    "print(a.size())\n",
    "print(torch.cat([x, y], dim=1))\n",
    "b=torch.cat([x,y], dim=1)\n",
    "print(b.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor([1, 4])\n",
    "y = torch.FloatTensor([2, 5])\n",
    "z = torch.FloatTensor([3, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 4.],\n",
      "        [2., 5.],\n",
      "        [3., 6.]])\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.stack([x, y, z]))\n",
    "print(torch.stack([x, y, z], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 4.],\n",
      "        [2., 5.],\n",
      "        [3., 6.]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.cat([x.unsqueeze(0), y.unsqueeze(0), z.unsqueeze(0)], dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ones and Zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [2., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([[0, 1, 2], [2, 1, 0]])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.ones_like(x))\n",
    "print(torch.zeros_like(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-place Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor([[1, 2], [3, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n"
     ]
    }
   ],
   "source": [
    "print(x.mul(2.))\n",
    "print(x)\n",
    "print(x.mul_(2.)) # 메모리를  새로 선언하지 않고 기존의 값에 넣음.\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "\n",
    "W = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "optimizer = torch.optim.SGD([W, b], lr=0.01)\n",
    "\n",
    "nb_epochs = 1000\n",
    "\n",
    "for epoch in arange(1, nb_epochs + 1):\n",
    "    \n",
    "    hypothesis = x_train * W + b\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    optimizer.zero_grad() # gradient 초기화\n",
    "    cost.backward() # gradient 계산\n",
    "    optimizer.step() # 계산된 편미분방향의 반대인 step 으로 개선.\n",
    "    \n",
    "## for문 range -> arange ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/10 W: 0.000, Cost: 4.666667\n",
      "Epoch    1/10 W: 1.400, Cost: 0.746667\n",
      "Epoch    2/10 W: 0.840, Cost: 0.119467\n",
      "Epoch    3/10 W: 1.064, Cost: 0.019115\n",
      "Epoch    4/10 W: 0.974, Cost: 0.003058\n",
      "Epoch    5/10 W: 1.010, Cost: 0.000489\n",
      "Epoch    6/10 W: 0.996, Cost: 0.000078\n",
      "Epoch    7/10 W: 1.002, Cost: 0.000013\n",
      "Epoch    8/10 W: 0.999, Cost: 0.000002\n",
      "Epoch    9/10 W: 1.000, Cost: 0.000000\n",
      "Epoch   10/10 W: 1.000, Cost: 0.000000\n"
     ]
    }
   ],
   "source": [
    "## non optim library\n",
    "\n",
    "# # 데이터\n",
    "# x_train = torch.FloatTensor([[0], [1], [2]])\n",
    "# y_train = torch.FloatTensor([[0], [1], [2]])\n",
    "# # 모델 초기화\n",
    "# W = torch.zeros(1)\n",
    "# # learning rate \n",
    "# lr = 0.1\n",
    "\n",
    "# nb_epochs = 10\n",
    "# for epoch in arange(nb_epochs + 1):\n",
    "    \n",
    "#     # H(x)\n",
    "#     hypothesis = x_train * W\n",
    "#     # Cost\n",
    "#     cost = torch.mean((hypothesis - y_train)**2)\n",
    "#     gradient = torch.sum((W * x_train - y_train) * x_train) ##\n",
    "    \n",
    "#     print('Epoch {:4d}/{} W: {:.3f}, Cost: {:.6f}'.format(epoch, nb_epochs, W.item(), cost.item()))\n",
    "    \n",
    "#     # Cost gradient 로 H(x) 개선\n",
    "#     W -= lr * gradient\n",
    "\n",
    "## including optim library\n",
    "\n",
    "\n",
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "# 모델 초기화\n",
    "W = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "## optimizer , learning 도 여기에 포함되어있음.\n",
    "optimizer = torch.optim.SGD([W], lr=0.15)\n",
    "\n",
    "nb_epochs = 10\n",
    "for epoch in arange(nb_epochs + 1):\n",
    "    \n",
    "    # H(x)\n",
    "    hypothesis = x_train * W\n",
    "    \n",
    "    # Cost\n",
    "    cost = torch.mean((hypothesis - y_train)**2)\n",
    "        \n",
    "    print('Epoch {:4d}/{} W: {:.3f}, Cost: {:.6f}'.format(epoch, nb_epochs, W.item(), cost.item()))\n",
    "    \n",
    "    # cost 로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[73, 85, 94],\n",
    "                            [72, 75, 84],\n",
    "                            [98, 45, 74],\n",
    "                            [75, 83, 68],\n",
    "                            [79, 88, 99],])\n",
    "y_train = torch.FloatTensor([[123],\n",
    "                            [143],\n",
    "                            [153],\n",
    "                            [126],\n",
    "                            [112]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    0/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 17481.400390625\n",
      "Epoch:    1/20 hypothesis: tensor([52.3126, 47.9851, 45.4645, 46.6727, 55.2458]) Cost: 7020.45703125\n",
      "Epoch:    2/20 hypothesis: tensor([84.6223, 77.6370, 73.6554, 75.5157, 89.3734]) Cost: 3020.271484375\n",
      "Epoch:    3/20 hypothesis: tensor([104.5667,  95.9556,  91.1676,  93.3369, 110.4462]) Cost: 1489.1002197265625\n",
      "Epoch:    4/20 hypothesis: tensor([116.8673, 107.2683, 102.0778, 104.3445, 123.4490]) Cost: 901.4947509765625\n",
      "Epoch:    5/20 hypothesis: tensor([124.4427, 114.2501, 108.9062, 111.1403, 131.4630]) Cost: 674.5035400390625\n",
      "Epoch:    6/20 hypothesis: tensor([129.0974, 118.5547, 113.2109, 115.3324, 136.3933]) Cost: 585.3509521484375\n",
      "Epoch:    7/20 hypothesis: tensor([131.9465, 121.2043, 115.9549, 117.9150, 139.4174]) Cost: 548.9025268554688\n",
      "Epoch:    8/20 hypothesis: tensor([133.6798, 122.8310, 117.7336, 119.5029, 141.2633]) Cost: 532.6240234375\n",
      "Epoch:    9/20 hypothesis: tensor([134.7233, 123.8253, 118.9150, 120.4757, 142.3809]) Cost: 524.0821533203125\n",
      "Epoch:   10/20 hypothesis: tensor([135.3407, 124.4288, 119.7267, 121.0686, 143.0485]) Cost: 518.5242919921875\n",
      "Epoch:   11/20 hypothesis: tensor([135.6948, 124.7907, 120.3094, 121.4266, 143.4380]) Cost: 514.1349487304688\n",
      "Epoch:   12/20 hypothesis: tensor([135.8862, 125.0034, 120.7500, 121.6394, 143.6557]) Cost: 510.2195739746094\n",
      "Epoch:   13/20 hypothesis: tensor([135.9773, 125.1239, 121.1022, 121.7627, 143.7674]) Cost: 506.5122985839844\n",
      "Epoch:   14/20 hypothesis: tensor([136.0066, 125.1874, 121.3993, 121.8306, 143.8137]) Cost: 502.9112854003906\n",
      "Epoch:   15/20 hypothesis: tensor([135.9977, 125.2159, 121.6619, 121.8643, 143.8196]) Cost: 499.37744140625\n",
      "Epoch:   16/20 hypothesis: tensor([135.9654, 125.2226, 121.9026, 121.8771, 143.8008]) Cost: 495.8951110839844\n",
      "Epoch:   17/20 hypothesis: tensor([135.9189, 125.2160, 122.1293, 121.8770, 143.7667]) Cost: 492.4586486816406\n",
      "Epoch:   18/20 hypothesis: tensor([135.8636, 125.2013, 122.3469, 121.8689, 143.7234]) Cost: 489.06475830078125\n",
      "Epoch:   19/20 hypothesis: tensor([135.8032, 125.1816, 122.5583, 121.8560, 143.6745]) Cost: 485.71221923828125\n",
      "Epoch:   20/20 hypothesis: tensor([135.7396, 125.1588, 122.7655, 121.8403, 143.6223]) Cost: 482.4004821777344\n"
     ]
    }
   ],
   "source": [
    "# non using nn.module\n",
    "W = torch.zeros((3, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "optimizer = torch.optim.SGD([W, b], lr=1e-5)\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in arange(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) \n",
    "    hypothesis = x_train.matmul(W) + b\n",
    "    \n",
    "    # cost\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    \n",
    "    # cost -> H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('Epoch: {:4d}/{} hypothesis: {} Cost: {}'.format(epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Cost: 36476.191406\n",
      "Epoch    1/20 Cost: 11434.528320\n",
      "Epoch    2/20 Cost: 3585.295654\n",
      "Epoch    3/20 Cost: 1124.977905\n",
      "Epoch    4/20 Cost: 353.798187\n",
      "Epoch    5/20 Cost: 112.073692\n",
      "Epoch    6/20 Cost: 36.305428\n",
      "Epoch    7/20 Cost: 12.555538\n",
      "Epoch    8/20 Cost: 5.110576\n",
      "Epoch    9/20 Cost: 2.776458\n",
      "Epoch   10/20 Cost: 2.044274\n",
      "Epoch   11/20 Cost: 1.814185\n",
      "Epoch   12/20 Cost: 1.741501\n",
      "Epoch   13/20 Cost: 1.718168\n",
      "Epoch   14/20 Cost: 1.710293\n",
      "Epoch   15/20 Cost: 1.707250\n",
      "Epoch   16/20 Cost: 1.705762\n",
      "Epoch   17/20 Cost: 1.704715\n",
      "Epoch   18/20 Cost: 1.703820\n",
      "Epoch   19/20 Cost: 1.702994\n",
      "Epoch   20/20 Cost: 1.702168\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultivariateLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# using nn.module\n",
    "# 데이터\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "# 모델 초기화\n",
    "model = MultivariateLinearRegressionModel()\n",
    "# optimizer 설정\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in arange(nb_epochs+1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 20번마다 로그 출력\n",
    "    print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "        epoch, nb_epochs, cost.item()\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.x_data = torch.FloatTensor([[73, 85, 94],\n",
    "                            [72, 75, 84],\n",
    "                            [98, 45, 74],\n",
    "                            [75, 83, 68],\n",
    "                            [79, 88, 99],])\n",
    "        self.y_data = torch.FloatTensor([[123],\n",
    "                                        [143],\n",
    "                                        [153],\n",
    "                                        [126],\n",
    "                                        [112]])\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.FloatTensor(self.x_data[idx])\n",
    "        y = torch.FloatTensor(self.y_data[idx])\n",
    "        \n",
    "        return x,y\n",
    "    \n",
    "dataset = CustomDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size = 2,\n",
    "    shuffle = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Batch 1/3 Cost: 32555.132812\n",
      "Epoch    0/20 Batch 2/3 Cost: 34658.738281\n",
      "Epoch    0/20 Batch 3/3 Cost: 31815.185547\n",
      "Epoch    1/20 Batch 1/3 Cost: 35347.015625\n",
      "Epoch    1/20 Batch 2/3 Cost: 32847.347656\n",
      "Epoch    1/20 Batch 3/3 Cost: 29854.203125\n",
      "Epoch    2/20 Batch 1/3 Cost: 31126.908203\n",
      "Epoch    2/20 Batch 2/3 Cost: 34366.523438\n",
      "Epoch    2/20 Batch 3/3 Cost: 35256.062500\n",
      "Epoch    3/20 Batch 1/3 Cost: 31126.908203\n",
      "Epoch    3/20 Batch 2/3 Cost: 32555.132812\n",
      "Epoch    3/20 Batch 3/3 Cost: 38878.843750\n",
      "Epoch    4/20 Batch 1/3 Cost: 30146.417969\n",
      "Epoch    4/20 Batch 2/3 Cost: 37067.453125\n",
      "Epoch    4/20 Batch 3/3 Cost: 31815.185547\n",
      "Epoch    5/20 Batch 1/3 Cost: 32847.347656\n",
      "Epoch    5/20 Batch 2/3 Cost: 35347.015625\n",
      "Epoch    5/20 Batch 3/3 Cost: 29854.203125\n",
      "Epoch    6/20 Batch 1/3 Cost: 30146.417969\n",
      "Epoch    6/20 Batch 2/3 Cost: 33535.625000\n",
      "Epoch    6/20 Batch 3/3 Cost: 38878.843750\n",
      "Epoch    7/20 Batch 1/3 Cost: 32555.132812\n",
      "Epoch    7/20 Batch 2/3 Cost: 35347.015625\n",
      "Epoch    7/20 Batch 3/3 Cost: 30438.630859\n",
      "Epoch    8/20 Batch 1/3 Cost: 32555.132812\n",
      "Epoch    8/20 Batch 2/3 Cost: 31126.908203\n",
      "Epoch    8/20 Batch 3/3 Cost: 38878.843750\n",
      "Epoch    9/20 Batch 1/3 Cost: 37067.453125\n",
      "Epoch    9/20 Batch 2/3 Cost: 31126.908203\n",
      "Epoch    9/20 Batch 3/3 Cost: 29854.203125\n",
      "Epoch   10/20 Batch 1/3 Cost: 32847.347656\n",
      "Epoch   10/20 Batch 2/3 Cost: 30834.695312\n",
      "Epoch   10/20 Batch 3/3 Cost: 38878.843750\n",
      "Epoch   11/20 Batch 1/3 Cost: 34366.523438\n",
      "Epoch   11/20 Batch 2/3 Cost: 31126.908203\n",
      "Epoch   11/20 Batch 3/3 Cost: 35256.062500\n",
      "Epoch   12/20 Batch 1/3 Cost: 34658.738281\n",
      "Epoch   12/20 Batch 2/3 Cost: 32555.132812\n",
      "Epoch   12/20 Batch 3/3 Cost: 31815.185547\n",
      "Epoch   13/20 Batch 1/3 Cost: 32847.347656\n",
      "Epoch   13/20 Batch 2/3 Cost: 30834.695312\n",
      "Epoch   13/20 Batch 3/3 Cost: 38878.843750\n",
      "Epoch   14/20 Batch 1/3 Cost: 34366.523438\n",
      "Epoch   14/20 Batch 2/3 Cost: 31126.908203\n",
      "Epoch   14/20 Batch 3/3 Cost: 35256.062500\n",
      "Epoch   15/20 Batch 1/3 Cost: 31126.908203\n",
      "Epoch   15/20 Batch 2/3 Cost: 37067.453125\n",
      "Epoch   15/20 Batch 3/3 Cost: 29854.203125\n",
      "Epoch   16/20 Batch 1/3 Cost: 32847.347656\n",
      "Epoch   16/20 Batch 2/3 Cost: 30834.695312\n",
      "Epoch   16/20 Batch 3/3 Cost: 38878.843750\n",
      "Epoch   17/20 Batch 1/3 Cost: 37067.453125\n",
      "Epoch   17/20 Batch 2/3 Cost: 30146.417969\n",
      "Epoch   17/20 Batch 3/3 Cost: 31815.185547\n",
      "Epoch   18/20 Batch 1/3 Cost: 35347.015625\n",
      "Epoch   18/20 Batch 2/3 Cost: 30146.417969\n",
      "Epoch   18/20 Batch 3/3 Cost: 35256.062500\n",
      "Epoch   19/20 Batch 1/3 Cost: 34366.523438\n",
      "Epoch   19/20 Batch 2/3 Cost: 33535.625000\n",
      "Epoch   19/20 Batch 3/3 Cost: 30438.630859\n",
      "Epoch   20/20 Batch 1/3 Cost: 30834.695312\n",
      "Epoch   20/20 Batch 2/3 Cost: 34658.738281\n",
      "Epoch   20/20 Batch 3/3 Cost: 35256.062500\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 20\n",
    "for epoch in arange(nb_epochs + 1):\n",
    "    for batch_idx, samples in enumerate(dataloader):\n",
    "        x_train, y_train = samples\n",
    "        # H(x)\n",
    "        prediction = model(x_train)\n",
    "        \n",
    "        # cost\n",
    "        cost = F.mse_loss(prediction, y_train)\n",
    "        \n",
    "        # cost -> H(x)\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(epoch, nb_epochs, batch_idx+1, len(dataloader), cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd5f62bdf30>"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[1,2], [2,3], [3,4], [4,5], [5,6], [6,7]]\n",
    "y_data = [[0], [1], [1], [1], [0], [0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.FloatTensor(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 2])\n",
      "torch.Size([6, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.zeros((2, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "hypothesis = 1 / (1 + torch.exp(-(x_train.matmul(W) + b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000]], grad_fn=<MulBackward0>)\n",
      "torch.Size([6, 1])\n"
     ]
    }
   ],
   "source": [
    "print(hypothesis)\n",
    "print(hypothesis.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000]], grad_fn=<SigmoidBackward>)\n",
      "torch.Size([6, 1])\n"
     ]
    }
   ],
   "source": [
    "hypothesis = torch.sigmoid(x_train.matmul(W) + b)\n",
    "\n",
    "print(hypothesis)\n",
    "print(hypothesis.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6931],\n",
      "        [0.6931],\n",
      "        [0.6931],\n",
      "        [0.6931],\n",
      "        [0.6931],\n",
      "        [0.6931]], grad_fn=<NegBackward>)\n",
      "tensor(0.6931, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# sample -(y_train[0] * torch.log(hypothesis[0]) + (1 - y_train[0]) * (torch.log(1 - hypothesis[0]))\n",
    "losses = -(y_train * torch.log(hypothesis) + \n",
    "           (1 - y_train) * torch.log(1 - hypothesis))\n",
    "print(losses)\n",
    "##\n",
    "cost = losses.mean()\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6931, grad_fn=<BinaryCrossEntropyBackward>)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same the above equation\n",
    "F.binary_cross_entropy(hypothesis, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whole Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 cost: 0.693147\n",
      "Epoch  100/1000 cost: 1.208955\n",
      "Epoch  200/1000 cost: 0.900994\n",
      "Epoch  300/1000 cost: 1.785612\n",
      "Epoch  400/1000 cost: 3.589777\n",
      "Epoch  500/1000 cost: 1.367346\n",
      "Epoch  600/1000 cost: 0.875838\n",
      "Epoch  700/1000 cost: 0.888636\n",
      "Epoch  800/1000 cost: 1.976118\n",
      "Epoch  900/1000 cost: 1.202257\n",
      "Epoch 1000/1000 cost: 0.894779\n"
     ]
    }
   ],
   "source": [
    "# 모델 초기화\n",
    "W = torch.zeros((2, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in arange(nb_epochs + 1):\n",
    "    \n",
    "    # Cost\n",
    "    hypothesis = torch.sigmoid(x_train.matmul(W) + b)\n",
    "    cost = F.binary_cross_entropy(hypothesis, y_train)\n",
    "    \n",
    "    # Cost -> H(x)\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # log\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} cost: {:.6f}'.format(\n",
    "        epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9449],\n",
      "        [0.7264],\n",
      "        [0.2913],\n",
      "        [0.0598],\n",
      "        [0.0098]], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "hypothesis = torch.sigmoid(x_train.matmul(W) + b)\n",
    "print(hypothesis[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False]])\n"
     ]
    }
   ],
   "source": [
    "prediction = hypothesis >= torch.FloatTensor([0.5])\n",
    "print(prediction[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9449],\n",
       "        [0.7264],\n",
       "        [0.2913],\n",
       "        [0.0598],\n",
       "        [0.0098]], grad_fn=<SliceBackward>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(hypothesis[:5])\n",
    "display(prediction[:5])\n",
    "display(y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True]])\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = prediction.float() == y_train\n",
    "print(correct_prediction[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higher Implementation with Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BinaryClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/100 Cost: 0.974912 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 4.857275 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 2.046420 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 6.185209 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 0.728534 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 2.653549 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 4.573590 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 4.535166 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 2.344078 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 6.065827 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 0.743901 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 2.961812 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 4.130284 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 4.823746 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 1.915762 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 6.204871 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 0.668050 Accuracy 83.33%\n",
      "Epoch 1000/100 Cost: 1.326213 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 5.031044 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 4.094687 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 2.720934 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 5.796669 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 0.838237 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 3.842508 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 2.957693 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 5.611610 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 0.950355 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 4.588771 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 2.024225 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 6.116895 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 0.683103 Accuracy 83.33%\n",
      "Epoch 1000/100 Cost: 0.980567 Accuracy 33.33%\n",
      "Epoch 1000/100 Cost: 4.063237 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 4.753998 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 1.787879 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 6.133296 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 0.687453 Accuracy 83.33%\n",
      "Epoch 1000/100 Cost: 0.782894 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 2.769718 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 5.686533 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 0.845003 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 3.416431 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 3.237502 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 5.330604 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 1.101362 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 5.034695 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 1.375874 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 5.763010 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 0.785009 Accuracy 66.67%\n",
      "Epoch 1000/100 Cost: 2.193270 Accuracy 33.33%\n",
      "Epoch 1000/100 Cost: 4.183151 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 4.574210 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 1.826731 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 6.027020 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 0.732632 Accuracy 83.33%\n",
      "Epoch 1000/100 Cost: 0.761154 Accuracy 66.67%\n",
      "Epoch 1000/100 Cost: 1.677232 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 5.996397 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 0.733950 Accuracy 83.33%\n",
      "Epoch 1000/100 Cost: 0.822540 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 2.529312 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 5.764253 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 0.782749 Accuracy 83.33%\n",
      "Epoch 1000/100 Cost: 1.845971 Accuracy 33.33%\n",
      "Epoch 1000/100 Cost: 4.256535 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 4.495284 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 1.866267 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 5.994957 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 0.747183 Accuracy 83.33%\n",
      "Epoch 1000/100 Cost: 0.767726 Accuracy 66.67%\n",
      "Epoch 1000/100 Cost: 1.496040 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 5.812710 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 0.771334 Accuracy 83.33%\n",
      "Epoch 1000/100 Cost: 1.366454 Accuracy 33.33%\n",
      "Epoch 1000/100 Cost: 4.118199 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 4.585159 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 1.735386 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 5.949127 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 0.758689 Accuracy 83.33%\n",
      "Epoch 1000/100 Cost: 0.806038 Accuracy 66.67%\n",
      "Epoch 1000/100 Cost: 1.966058 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 5.969330 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 0.755907 Accuracy 83.33%\n",
      "Epoch 1000/100 Cost: 0.781444 Accuracy 66.67%\n",
      "Epoch 1000/100 Cost: 1.595865 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 5.883835 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 0.762054 Accuracy 83.33%\n",
      "Epoch 1000/100 Cost: 0.991709 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 3.351297 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 5.156799 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 1.146922 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 4.801661 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 1.469714 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 5.677137 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 0.815370 Accuracy 83.33%\n",
      "Epoch 1000/100 Cost: 1.668821 Accuracy 33.33%\n",
      "Epoch 1000/100 Cost: 4.067335 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 4.587912 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 1.653962 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 5.832338 Accuracy 50.00%\n",
      "Epoch 1000/100 Cost: 0.789621 Accuracy 83.33%\n"
     ]
    }
   ],
   "source": [
    "# optimizer \n",
    "optimizer = optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "nb_epochs = 100\n",
    "for epochs in arange(nb_epochs + 1):\n",
    "    \n",
    "    # h(x)\n",
    "    hypothesis = model(x_train)\n",
    "    \n",
    "    # cost\n",
    "    cost = F.binary_cross_entropy(hypothesis, y_train)\n",
    "    \n",
    "    # cost -> H(x)\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # each 20 epochs then print\n",
    "    if epoch % 20 == 0:\n",
    "        prediction = hypothesis >= torch.FloatTensor([0.5])\n",
    "        correct_prediction = prediction.float() == y_train\n",
    "        accuracy = correct_prediction.sum().item() / len(correct_prediction)\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f} Accuracy {:2.2f}%'.format(\n",
    "        epoch, nb_epochs, cost.item(), accuracy * 100))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd5f62bdf30>"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete Probability Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "연속형 / 이산형 확률변수에 대한 개요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max / softmax 에 대한 개요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4869, 0.1052, 0.5883, 0.1161, 0.4949],\n",
       "        [0.2824, 0.5899, 0.8105, 0.2512, 0.6307],\n",
       "        [0.5403, 0.8033, 0.7781, 0.4966, 0.8888]], requires_grad=True)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2228, 0.1521, 0.2466, 0.1538, 0.2246],\n",
      "        [0.1552, 0.2111, 0.2632, 0.1505, 0.2199],\n",
      "        [0.1683, 0.2189, 0.2134, 0.1611, 0.2384]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "z = torch.rand(3, 5, requires_grad=True)\n",
    "hypothesis = F.softmax(z, dim=1)\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 2, 0])\n"
     ]
    }
   ],
   "source": [
    "y = torch.randint(5, (3,)).long()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 1.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_one_hot = torch.zeros_like(hypothesis)\n",
    "y_one_hot.scatter_(1, y.unsqueeze(1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5])"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_one_hot.shape)\n",
    "torch.log(hypothesis).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 1.4934],\n",
       "        [0.0000, 0.0000, 1.3347, 0.0000, 0.0000],\n",
       "        [1.7823, 0.0000, 0.0000, 0.0000, 0.0000]], grad_fn=<MulBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.7823, 0.0000, 1.3347, 0.0000, 1.4934], grad_fn=<SumBackward1>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.4934, 1.3347, 1.7823], grad_fn=<SumBackward1>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.5368, grad_fn=<MeanBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(y_one_hot * -torch.log(hypothesis))\n",
    "display((y_one_hot * -torch.log(hypothesis)).sum(dim=0))\n",
    "display((y_one_hot * -torch.log(hypothesis)).sum(dim=1))\n",
    "display((y_one_hot * -torch.log(hypothesis)).sum(dim=1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5368, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost = (y_one_hot * -torch.log(hypothesis)).sum(dim=1).mean()\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5014, -1.8830, -1.3999, -1.8721, -1.4934],\n",
       "        [-1.8628, -1.5553, -1.3347, -1.8940, -1.5145],\n",
       "        [-1.7823, -1.5193, -1.5444, -1.8260, -1.4338]],\n",
       "       grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# low level\n",
    "torch.log(F.softmax(z, dim=1))\n",
    "\n",
    "# high level\n",
    "F.log_softmax(z, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5368, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# low level\n",
    "(y_one_hot * -torch.log(F.softmax(z, dim=1))).sum(dim=1).mean()\n",
    "\n",
    "# high level\n",
    "F.nll_loss(F.log_softmax(z, dim=1), y)\n",
    "\n",
    "# nll -> Negative Log Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5368, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(z,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Low-level Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "x_train = (np.random.rand(8,4)*10).round()\n",
    "y_train = (np.random.rand(1,8)*10).round()\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.LongTensor(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.,  7.,  6.,  5.],\n",
       "        [ 4.,  6.,  4.,  9.],\n",
       "        [10.,  4.,  8.,  5.],\n",
       "        [ 6.,  9.,  1.,  1.],\n",
       "        [ 0.,  8.,  8.,  9.],\n",
       "        [10.,  8.,  5.,  8.],\n",
       "        [ 1.,  6.,  1.,  9.],\n",
       "        [ 5.,  4.,  3.,  8.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[5, 6, 0, 6, 6, 6, 9, 7]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4])\n",
      "torch.Size([1, 8])\n"
     ]
    }
   ],
   "source": [
    "# display(x_train)\n",
    "# display(y_train)\n",
    "# print(x_train.shape)\n",
    "# print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "index 5 is out of bounds for dimension 1 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-379-149631b36b59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mhypothesis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0my_one_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypothesis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0my_one_hot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_one_hot\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypothesis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: index 5 is out of bounds for dimension 1 with size 3"
     ]
    }
   ],
   "source": [
    "# 모델 초기화\n",
    "W = torch.zeros((4, 3), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.SGD([W, b], lr=0.1)\n",
    "\n",
    "nb_epochs=1000\n",
    "for epochs in arange(nb_epochs + 1):\n",
    "    \n",
    "    hypothesis = F.softmax(x_train.matmul(W) + b, dim=1)\n",
    "    y_one_hot = torch.zeros_like(hypothesis)\n",
    "    y_one_hot.scatter_(1, y_train.unsqueeze(1), 1)\n",
    "    cost = (y_one_hot * -torch.log(F.softmax(hypothesis, dim=1))).sum(dim=1)\n",
    "    \n",
    "    # cost -> H(x)\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx_train == x_train\n",
    "xx_train.shape == x_train.shape\n",
    "y_train == yy_train\n",
    "y_train.shape == yy_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 1.098612\n",
      "Epoch  100/1000 Cost: 0.901535\n",
      "Epoch  200/1000 Cost: 0.839114\n",
      "Epoch  300/1000 Cost: 0.807826\n",
      "Epoch  400/1000 Cost: 0.788472\n",
      "Epoch  500/1000 Cost: 0.774822\n",
      "Epoch  600/1000 Cost: 0.764449\n",
      "Epoch  700/1000 Cost: 0.756191\n",
      "Epoch  800/1000 Cost: 0.749398\n",
      "Epoch  900/1000 Cost: 0.743671\n",
      "Epoch 1000/1000 Cost: 0.738749\n"
     ]
    }
   ],
   "source": [
    "x_train = [[1, 2, 1, 1],\n",
    "           [2, 1, 3, 2],\n",
    "           [3, 1, 3, 4],\n",
    "           [4, 1, 5, 5],\n",
    "           [1, 7, 5, 5],\n",
    "           [1, 2, 5, 6],\n",
    "           [1, 6, 6, 6],\n",
    "           [1, 7, 7, 7]]\n",
    "y_train = [2, 2, 2, 1, 1, 1, 0, 0]\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "\n",
    "# 모델 초기화\n",
    "W = torch.zeros((4, 3), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in arange(nb_epochs + 1):\n",
    "\n",
    "    # Cost 계산 (1)\n",
    "    hypothesis = F.softmax(x_train.matmul(W) + b, dim=1) # or .mm or @\n",
    "    y_one_hot = torch.zeros_like(hypothesis)\n",
    "    y_one_hot.scatter_(1, y_train.unsqueeze(1), 1)\n",
    "    cost = (y_one_hot * -torch.log(F.softmax(hypothesis, dim=1))).sum(dim=1).mean()\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with F.corss_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 1.098612\n",
      "Epoch  100/1000 Cost: 0.761050\n",
      "Epoch  200/1000 Cost: 0.689991\n",
      "Epoch  300/1000 Cost: 0.643229\n",
      "Epoch  400/1000 Cost: 0.604117\n",
      "Epoch  500/1000 Cost: 0.568256\n",
      "Epoch  600/1000 Cost: 0.533922\n",
      "Epoch  700/1000 Cost: 0.500291\n",
      "Epoch  800/1000 Cost: 0.466908\n",
      "Epoch  900/1000 Cost: 0.433507\n",
      "Epoch 1000/1000 Cost: 0.399962\n"
     ]
    }
   ],
   "source": [
    "# 모델 초기화\n",
    "W = torch.zeros((4, 3), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in arange(nb_epochs + 1):\n",
    "    \n",
    "    # Cost 계산\n",
    "    z = x_train.matmul(W) + b\n",
    "    cost = F.cross_entropy(z, y_train)\n",
    "    \n",
    "    # cost -> H(x)\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-level Implementation with nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassifierModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(4, 3) # 4개의 값을 받아서 3개의 값을 내놓음.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftmaxClassifierModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 cost: 1.106180\n",
      "Epoch  100/1000 cost: 0.720618\n",
      "Epoch  200/1000 cost: 0.637684\n",
      "Epoch  300/1000 cost: 0.579574\n",
      "Epoch  400/1000 cost: 0.528705\n",
      "Epoch  500/1000 cost: 0.480843\n",
      "Epoch  600/1000 cost: 0.434375\n",
      "Epoch  700/1000 cost: 0.388519\n",
      "Epoch  800/1000 cost: 0.342926\n",
      "Epoch  900/1000 cost: 0.297923\n",
      "Epoch 1000/1000 cost: 0.257454\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in arange(nb_epochs + 1):\n",
    "    \n",
    "    # H(x)\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    # cost\n",
    "    cost = F.cross_entropy(prediction, y_train)\n",
    "    \n",
    "    # cost -> H(x)\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} cost: {:6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "\n",
    "- More Data\n",
    "\n",
    "\n",
    "- Less features\n",
    "\n",
    "\n",
    "- Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "- Early Stopping\n",
    "\n",
    "\n",
    "- Reducing Network Size\n",
    "\n",
    "\n",
    "- Weight Decay\n",
    "\n",
    "\n",
    "- Dropout\n",
    "\n",
    "\n",
    "- Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Approach to Train DNN\n",
    "\n",
    "1. Make a neural network architecture.\n",
    "\n",
    "\n",
    "2. Train and check that model is over-fitted.\n",
    "\n",
    "    a) If it is not, increase the model size (deeper and wider)\n",
    "    \n",
    "    b) If it is, add regularization, such as drop-out, batch_normalization\n",
    "    \n",
    "\n",
    "3. Repeat from step-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd5f62bdf30>"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2020)\n",
    "x_train = torch.FloatTensor((np.random.rand(8, 3)*10).round().astype(int))\n",
    "y_train = torch.LongTensor([2, 2, 2, 1, 1, 1, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.FloatTensor([[2, 1, 1], [3, 1, 2], [3, 3, 4]])\n",
    "y_test = torch.LongTensor([2, 2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassifierModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 3)\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftmaxClassifierModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, x_train, y_train):\n",
    "    nb_epochs = 20\n",
    "    for epoch in arange(nb_epochs):\n",
    "\n",
    "        # H(x) 계산\n",
    "        prediction = model(x_train)\n",
    "\n",
    "        # cost 계산\n",
    "        cost = F.cross_entropy(prediction, y_train)\n",
    "\n",
    "        # cost로 H(x) 개선\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test ( Validation )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, optimizer, x_test, y_test):\n",
    "    prediction = model(x_test)\n",
    "    predicted_classes = prediction.max(1)[1]\n",
    "    correct_count = (predicted_classes == y_test).sum().item()\n",
    "    cost = F.cross_entropy(prediction, y_test)\n",
    "    \n",
    "    print('Accuracy: {}% Cost: {:.6f}'.format(\n",
    "        correct_count / len(y_test) * 100, cost.item()\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Cost: 809201.125000\n",
      "Epoch    1/20 Cost: 948511.625000\n",
      "Epoch    2/20 Cost: 148469.781250\n",
      "Epoch    3/20 Cost: 138872.312500\n",
      "Epoch    4/20 Cost: 26594.777344\n",
      "Epoch    5/20 Cost: 221221.562500\n",
      "Epoch    6/20 Cost: 720689.562500\n",
      "Epoch    7/20 Cost: 266843.875000\n",
      "Epoch    8/20 Cost: 189094.781250\n",
      "Epoch    9/20 Cost: 52934.812500\n",
      "Epoch   10/20 Cost: 67219.781250\n",
      "Epoch   11/20 Cost: 69968.882812\n",
      "Epoch   12/20 Cost: 414682.500000\n",
      "Epoch   13/20 Cost: 953195.625000\n",
      "Epoch   14/20 Cost: 1211724.625000\n",
      "Epoch   15/20 Cost: 166786.656250\n",
      "Epoch   16/20 Cost: 59407.296875\n",
      "Epoch   17/20 Cost: 381466.375000\n",
      "Epoch   18/20 Cost: 845689.625000\n",
      "Epoch   19/20 Cost: 266786.656250\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 66.66666666666666% Cost: 0.759458\n"
     ]
    }
   ],
   "source": [
    "test(model, optimizer, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftmaxClassifierModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### learning rate 가 너무 클 때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftmaxClassifierModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=1e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Cost: 1.896620\n",
      "Epoch    1/20 Cost: 1272039.750000\n",
      "Epoch    2/20 Cost: 2769402.500000\n",
      "Epoch    3/20 Cost: 1324117.625000\n",
      "Epoch    4/20 Cost: 732977.250000\n",
      "Epoch    5/20 Cost: 2317840.000000\n",
      "Epoch    6/20 Cost: 1282894.500000\n",
      "Epoch    7/20 Cost: 684057.062500\n",
      "Epoch    8/20 Cost: 583175.062500\n",
      "Epoch    9/20 Cost: 710953.000000\n",
      "Epoch   10/20 Cost: 857302.250000\n",
      "Epoch   11/20 Cost: 316649.500000\n",
      "Epoch   12/20 Cost: 134790.375000\n",
      "Epoch   13/20 Cost: 133231.875000\n",
      "Epoch   14/20 Cost: 462182.125000\n",
      "Epoch   15/20 Cost: 742839.812500\n",
      "Epoch   16/20 Cost: 1520328.125000\n",
      "Epoch   17/20 Cost: 559456.937500\n",
      "Epoch   18/20 Cost: 240410.515625\n",
      "Epoch   19/20 Cost: 346421.500000\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### learning rate 가 너무 작을 때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftmaxClassifierModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Cost: 3.360192\n",
      "Epoch    1/20 Cost: 3.360192\n",
      "Epoch    2/20 Cost: 3.360192\n",
      "Epoch    3/20 Cost: 3.360192\n",
      "Epoch    4/20 Cost: 3.360192\n",
      "Epoch    5/20 Cost: 3.360192\n",
      "Epoch    6/20 Cost: 3.360192\n",
      "Epoch    7/20 Cost: 3.360192\n",
      "Epoch    8/20 Cost: 3.360192\n",
      "Epoch    9/20 Cost: 3.360192\n",
      "Epoch   10/20 Cost: 3.360192\n",
      "Epoch   11/20 Cost: 3.360192\n",
      "Epoch   12/20 Cost: 3.360192\n",
      "Epoch   13/20 Cost: 3.360192\n",
      "Epoch   14/20 Cost: 3.360192\n",
      "Epoch   15/20 Cost: 3.360192\n",
      "Epoch   16/20 Cost: 3.360192\n",
      "Epoch   17/20 Cost: 3.360192\n",
      "Epoch   18/20 Cost: 3.360192\n",
      "Epoch   19/20 Cost: 3.360192\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### learning rate가 적절할 때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftmaxClassifierModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Cost: 2.182966\n",
      "Epoch    1/20 Cost: 1.544655\n",
      "Epoch    2/20 Cost: 0.991795\n",
      "Epoch    3/20 Cost: 0.931347\n",
      "Epoch    4/20 Cost: 1.021245\n",
      "Epoch    5/20 Cost: 0.909969\n",
      "Epoch    6/20 Cost: 0.793979\n",
      "Epoch    7/20 Cost: 0.805483\n",
      "Epoch    8/20 Cost: 0.703795\n",
      "Epoch    9/20 Cost: 0.697935\n",
      "Epoch   10/20 Cost: 0.633792\n",
      "Epoch   11/20 Cost: 0.610649\n",
      "Epoch   12/20 Cost: 0.569784\n",
      "Epoch   13/20 Cost: 0.540126\n",
      "Epoch   14/20 Cost: 0.514346\n",
      "Epoch   15/20 Cost: 0.488174\n",
      "Epoch   16/20 Cost: 0.471633\n",
      "Epoch   17/20 Cost: 0.452177\n",
      "Epoch   18/20 Cost: 0.440520\n",
      "Epoch   19/20 Cost: 0.426650\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = x_train.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = x_train.std(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_x_train = (x_train - mu) / sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultivariateLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultivariateLinearRegressionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, x_train, y_train):\n",
    "    nb_epochs = 20\n",
    "    for epoch in arange(nb_epochs):\n",
    "\n",
    "        # H(x) 계산\n",
    "        prediction = model(x_train)\n",
    "\n",
    "        # cost 계산\n",
    "        cost = F.mse_loss(prediction, y_train)\n",
    "\n",
    "        # cost로 H(x) 개선\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Cost: 29579.099609\n",
      "Epoch    1/20 Cost: 18780.074219\n",
      "Epoch    2/20 Cost: 11975.608398\n",
      "Epoch    3/20 Cost: 7651.863281\n",
      "Epoch    4/20 Cost: 4893.715820\n",
      "Epoch    5/20 Cost: 3131.131104\n",
      "Epoch    6/20 Cost: 2003.836670\n",
      "Epoch    7/20 Cost: 1282.585205\n",
      "Epoch    8/20 Cost: 821.041992\n",
      "Epoch    9/20 Cost: 525.666687\n",
      "Epoch   10/20 Cost: 336.625061\n",
      "Epoch   11/20 Cost: 215.634201\n",
      "Epoch   12/20 Cost: 138.194412\n",
      "Epoch   13/20 Cost: 88.627426\n",
      "Epoch   14/20 Cost: 56.899132\n",
      "Epoch   15/20 Cost: 36.587547\n",
      "Epoch   16/20 Cost: 23.583166\n",
      "Epoch   17/20 Cost: 15.255445\n",
      "Epoch   18/20 Cost: 9.921062\n",
      "Epoch   19/20 Cost: 6.502402\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, norm_x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "428.8px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
